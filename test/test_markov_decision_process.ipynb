{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82833b0b-e108-456a-b605-2f8b8a0a4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "sys.path.append('..')  #let me import stuff from the path above\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Generic, Callable, TypeVar, Iterable,\\\n",
    "Optional, Mapping, Tuple\n",
    "from collections import defaultdict\n",
    "from distribution import Distribution, Categorical, SampledDistribution, Constant, \\\n",
    "FiniteDistribution\n",
    "from scipy.stats import poisson\n",
    "from markov_decision_process import *\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "    \n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "\n",
    "InvOrerMapping = Mapping[\n",
    "    InventoryState,\n",
    "    Mapping[int, Categorical[Tuple[InventoryState, float]]]\n",
    "]\n",
    "\n",
    "class SimpleInventoryMDPCap(FiniteMarkovDecisionProcess[InventoryState, int]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda:float,\n",
    "        holding_cost:float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "        \n",
    "    def get_action_transition_reward_map(self) -> InvOrderMapping:\n",
    "        d: Dict[InventoryState, Dict[int, Categorical[Tuple[InventoryState, float]]]] = {}\n",
    "        \n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity + 1 - alpha):\n",
    "                state: InventoryState = InventoryState(alpha, beta)\n",
    "                ip: int = state.inventory_position()\n",
    "                base_reward: float = - self.holding_cost * alpha\n",
    "                d1: Dict[int, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "                \n",
    "                for order in range(self.capacity - ip + 1):\n",
    "                    sr_probs_dict: Dict[tuple[InventoryState, float], float] =\\\n",
    "                        {(InventoryState(ip - i, order), base_reward):\n",
    "                         self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "                    \n",
    "                    probability: float = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                    reward: float = base_reward - self.stockout_cost *\\\n",
    "                        (probability * (self.poisson_lambda - ip) +\n",
    "                         ip * self.poisson_distr.pmf(ip))\n",
    "                    sr_probs_dict[(WInventoryState(0, order), reward)] = \\\n",
    "                        probability\n",
    "                    d1[order] = Categorical(sr_probs_dict)    \n",
    "                d[state] = d1\n",
    "                \n",
    "        return d\n",
    "        \n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] = \\\n",
    "    SimpleInventoryMDPCap(\n",
    "        capacity=user_capacity,\n",
    "        poisson_lambda=user_poisson_lambda,\n",
    "        holding_cost=user_holding_cost,\n",
    "        stockout_cost=user_stockout_cost\n",
    ")\n",
    "fdp: FiniteDeterministicPolicy[InventoryState, int] =\\\n",
    "    FiniteDeterministicPolicy(\n",
    "        {InventoryState(alpha,beta): user_capacity - (alpha + beta)\n",
    "         for alpha in range(user_capacity + 1)\n",
    "         for beta in range(user_capacity + 1 - alpha)}\n",
    ")\n",
    "\n",
    "implied_mrp: FiniteMarkovRewardProcess[InventoryState] = \\\n",
    "    si_mdp.apply_finite_policy(fdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dac9c68f-77f4-44ca-af23-5c2ade44b93e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -10.0,\n",
       " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -3.927342083242136,\n",
       " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -3.8382469453207233,\n",
       " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -4.927342083242136,\n",
       " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -4.838246945320723,\n",
       " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -5.838246945320723}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dynamic_programming as dp\n",
    "import importlib\n",
    "importlib.reload(dp) \n",
    "\n",
    "dp.evaluate_mrp_result(implied_mrp,0.9)\n",
    "# si_mdp.mapping[NonTerminal(InventoryState(0,1))][1].expectation(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49c48121-b52f-4223-b8c1-d31bce4ac08b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -10.0,\n",
       "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -3.927342083242136,\n",
       "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -3.8382469453207233,\n",
       "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -4.927342083242136,\n",
       "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -4.838246945320723,\n",
       "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -5.838246945320723},\n",
       " For State InventoryState(on_hand=0, on_order=0): Do Action 0\n",
       " For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
       " For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
       " For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
       " For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
       " For State InventoryState(on_hand=2, on_order=0): Do Action 0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.policy_iteration_result(si_mdp,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f77a579e-7ff9-4c63-b247-6edd6caa6883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -10.0,\n",
       "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -3.927342083242136,\n",
       "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -3.8382469453207237,\n",
       "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -4.927342083242136,\n",
       "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -4.838246945320723,\n",
       "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -5.838246945320723},\n",
       " For State InventoryState(on_hand=0, on_order=0): Do Action 0\n",
       " For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
       " For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
       " For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
       " For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
       " For State InventoryState(on_hand=2, on_order=0): Do Action 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.value_iteration_result(si_mdp,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "232522f8-229b-43cf-836f-ad0e4f2a46fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InventoryState' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jy/62vd459n2cjb81kdlvhf6vp00000gn/T/ipykernel_61812/484992898.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_finite_horizon_MDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msi_mdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/MSBD HW/6000M Reinforcement Learning with Applications in Finance/finite_horizon.py\u001b[0m in \u001b[0;36munwrap_finite_horizon_MDP\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         ),\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     )]\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSBD HW/6000M Reinforcement Learning with Applications in Finance/finite_horizon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m     return [{NonTerminal(s.state) : without_time(\n\u001b[1;32m     75\u001b[0m         \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNonTerminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     ) for s in states} for _, states in groupby( #Groupby -> {t=0: [State_0,State_1,State_2], t=1:[State_3,State_4,State_5]}\n\u001b[0m\u001b[1;32m     77\u001b[0m         sorted(\n\u001b[1;32m     78\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_terminal_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSBD HW/6000M Reinforcement Learning with Applications in Finance/finite_horizon.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m     return [{NonTerminal(s.state) : without_time(\n\u001b[1;32m     75\u001b[0m         \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNonTerminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     ) for s in states} for _, states in groupby( #Groupby -> {t=0: [State_0,State_1,State_2], t=1:[State_3,State_4,State_5]}\n\u001b[0m\u001b[1;32m     77\u001b[0m         sorted(\n\u001b[1;32m     78\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_terminal_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSBD HW/6000M Reinforcement Learning with Applications in Finance/finite_horizon.py\u001b[0m in \u001b[0;36mwithout_time\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#wrap all states with Terminal or NonTerminal, strips time away from state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithout_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActionMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWithTime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mActionMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msr_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_without_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_distr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     return [{NonTerminal(s.state) : without_time(\n",
      "\u001b[0;32m~/Documents/MSBD HW/6000M Reinforcement Learning with Applications in Finance/finite_horizon.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#wrap all states with Terminal or NonTerminal, strips time away from state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithout_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActionMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWithTime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mActionMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msr_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_without_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_distr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     return [{NonTerminal(s.state) : without_time(\n",
      "\u001b[0;32m~/Documents/MSBD HW/6000M Reinforcement Learning with Applications in Finance/distribution.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSBD HW/6000M Reinforcement Learning with Applications in Finance/finite_horizon.py\u001b[0m in \u001b[0;36msingle_without_time\u001b[0;34m(s_r)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNonTerminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             ret: Tuple[State[S], float] = (\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mNonTerminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0ms_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'InventoryState' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "import finite_horizon as fh\n",
    "import importlib\n",
    "importlib.reload(fh) \n",
    "fh.unwrap_finite_horizon_MDP(si_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcafd96c-1a4e-40b9-b8a7-1f895898526e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
