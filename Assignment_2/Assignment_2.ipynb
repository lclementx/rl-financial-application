{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203379bc-c0e9-4c32-b6de-534efb14ef07",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### MAIN: In a Binomial model of a single stock with non-zero interest rate, assume that we can hedge any fraction of a stock, use policy gradient to train the optimal policy of hedging an ATM American put option with maturity T = 10. WHEN do you early exercise the option? Is your solution same as what you obtain from delta hedging?\n",
    "\n",
    "#### OPTIONAL: For really advanced students: supppose the stock follows a GBM (Geometric Brownian Motion), construct an algorithm to train a Neural Network that hedges an ATM American Put Option\n",
    "\n",
    "#### OPTIONAL BONUS: After solving the optional question, sue the Soft Actor Critic algorithm in TF agent to solve the problem again in the colab.resarch.google.com environment. Compare your results\n",
    "\n",
    "#### ADVANCED EXTRA BONUS: Implement the GAC algorithm using the TF agent library and solve the optinal problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269007c7-475a-49e9-821b-ccf70276c46b",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b59a1a2-3c83-4409-96d1-fc6947dd9edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clemmie/opt/anaconda3/envs/Python3_7_Plus_R/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import MultivariateNormal, Normal\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.special import comb\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c11c39-72c8-4d97-a207-3ff119007479",
   "metadata": {},
   "source": [
    "### 10 Step American Put Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73297ed2-e76a-4a59-b005-c4946670fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,1)\n",
    "        self.tanh = nn.Tanh()\n",
    "            \n",
    "    def forward(self, state):\n",
    "        action = self.tanh(self.linear1(state))\n",
    "        action = self.tanh(self.linear2(action))\n",
    "        action = self.linear3(action)\n",
    "        return action    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim + action_dim,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,1)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, state, action):\n",
    "        input_tensor = torch.cat([state,action],1)\n",
    "        value = self.relu(self.linear1(input_tensor))\n",
    "        value = self.relu(self.linear2(value))\n",
    "        value = self.linear3(value)\n",
    "        return value\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, np.array([done]))\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "            \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DDPGAgent():\n",
    "    def __init__(self, state_dim, action_dim, \n",
    "                 actor_lr=1e-4, critic_lr=1e-3, \n",
    "                 gamma=0.99, tau=1e-2, max_memory=50000):\n",
    "        \n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.gamma=gamma\n",
    "        self.tau=tau\n",
    "        \n",
    "        ###Actors and Critics\n",
    "        self.actor = Actor(self.state_dim)\n",
    "        self.target_actor = Actor(self.state_dim)\n",
    "        self.critic = Critic(self.state_dim,self.action_dim)\n",
    "        self.target_critic = Critic(self.state_dim,self.action_dim)\n",
    "        \n",
    "        #Make sure the params are the same for both networks. from actor -> target_actor, critic -> target_critic\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        #For training:\n",
    "        self.replay_buffer = ReplayBuffer(max_memory)\n",
    "        self.critic_criterion = nn.MSELoss()\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        state.requires_grad=True\n",
    "        action = self.actor.forward(state)\n",
    "        action = action.detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, done = self.replay_buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        done = torch.FloatTensor(done)\n",
    "        ##Critic Update\n",
    "        # print(f'State: {states},Action: {actions}')\n",
    "        Q_values = self.critic.forward(states, actions.detach())\n",
    "        next_actions = self.target_actor.forward(next_states)\n",
    "        next_Q = self.target_critic.forward(next_states, next_actions.detach())\n",
    "        Q_targets = rewards + (1 - done) * self.gamma * next_Q\n",
    "        critic_loss = self.critic_criterion(Q_values,Q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        ####Policy Update\n",
    "        actor_loss = -self.critic.forward(states,self.actor.forward(states)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        ####Update Target Networks\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1-self.tau) * target_param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1-self.tau) * target_param.data)\n",
    "    \n",
    "class AmericanOptionHedgingEnvironment():\n",
    "    def __init__(self, strike_price,init_stock_price,up_factor,down_factor,risk_free_rate,termination_time):\n",
    "        self.strike_price=strike_price\n",
    "        self.init_stock_price=init_stock_price\n",
    "        self.up_factor=up_factor\n",
    "        self.down_factor=down_factor\n",
    "        self.risk_free_rate=risk_free_rate\n",
    "        self.hedge_portfolio_value=0\n",
    "        self.time_step = 0\n",
    "        self.termination_time=termination_time\n",
    "        self.risk_neutral_probability = (1 + risk_free_rate - down_factor) / (up_factor - down_factor)\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        if self.time_step == self.termination_time:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "    def reward_function(self,up_price,down_price,action,state):\n",
    "        up_payoff = max(self.strike_price - up_price,0)\n",
    "        down_payoff = max(self.strike_price - down_price,0)\n",
    "        up_hedge = action * (up_price - state) - up_payoff\n",
    "        down_hedge = action * (down_price - state) - down_payoff\n",
    "        return -abs(up_hedge - down_hedge)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.time_step = 0\n",
    "        init_state = np.array([self.init_stock_price,self.strike_price])\n",
    "        return init_state\n",
    "    \n",
    "    def step(self,state,action):\n",
    "        self.time_step += 1\n",
    "        up_price = self.up_factor * state[0].item()\n",
    "        down_price = self.down_factor * state[0].item()\n",
    "        next_price = np.random.choice([up_price, down_price],\n",
    "                                     p=[self.risk_neutral_probability,1-self.risk_neutral_probability])\n",
    "        next_state=np.array([next_price,self.strike_price])\n",
    "        reward = self.reward_function(up_price,down_price,action.item(),state[0].item())\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349da8e1-0faf-4373-846f-d0472a5ae9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0, State: [86.57685041 62.        ], Action: [-0.15305854], Reward: -1.8069990178792172\n",
      "Episode:500, State: [74.77091626 62.        ], Action: [0.2875404], Reward: -2.931771692936648\n",
      "Episode:1000, State: [64.57488222 62.        ], Action: [-0.22369316], Reward: -2.28078306588764\n",
      "Episode:1500, State: [64.57488222 62.        ], Action: [-0.24079085], Reward: -2.455111672977557\n",
      "Episode:2000, State: [86.57685041 62.        ], Action: [0.03827858], Reward: -0.4519143660980284\n",
      "Episode:2500, State: [100.24687942  62.        ], Action: [-0.00574679], Reward: -0.07855877247281931\n",
      "Episode:3000, State: [86.57685041 62.        ], Action: [0.03259825], Reward: -0.44561906649732663\n",
      "Episode:3500, State: [86.57685041 62.        ], Action: [-0.02111461], Reward: -0.24927773834369382\n",
      "Episode:4000, State: [64.57488222 62.        ], Action: [-0.01107196], Reward: -0.11289004090962049\n",
      "Episode:4500, State: [55.76921647 62.        ], Action: [-0.69671546], Reward: -0.09574003992743663\n",
      "Episode:5000, State: [64.57488222 62.        ], Action: [-0.67687515], Reward: -0.27044724324018476\n"
     ]
    }
   ],
   "source": [
    "state_dim=2\n",
    "action_dim=1\n",
    "strike_price=62.0\n",
    "init_stock_price=60.0\n",
    "up_factor=1.1\n",
    "down_factor=0.95\n",
    "risk_free_rate=0.03\n",
    "termination_time=10\n",
    "episodes=5000\n",
    "batch_size=64\n",
    "#https://arxiv.org/pdf/1911.11679.pdf - becareful of actor_learning_rate vs critic_learning_rate\n",
    "env = AmericanOptionHedgingEnvironment(strike_price,init_stock_price,up_factor,down_factor,risk_free_rate,termination_time)\n",
    "ddpg_agent = DDPGAgent(state_dim,action_dim,\n",
    "                       actor_lr=5e-5,critic_lr=2e-4)\n",
    "\n",
    "rewards = []\n",
    "for e in range(episodes+1):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for t in range(termination_time):\n",
    "        action = ddpg_agent.get_action(state)\n",
    "        noise = np.random.normal(0,1, size=action_dim) / (1 + e/100) #Progressively reduce noise as we learn\n",
    "        action = action + noise \n",
    "        next_state, reward = env.step(state,action)\n",
    "        ddpg_agent.replay_buffer.push(state,action,reward,next_state,env.is_terminal())\n",
    "        if len(ddpg_agent.replay_buffer) > batch_size:\n",
    "            ddpg_agent.update(batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "    if e % 500 == 0:\n",
    "        print(f'Episode:{e}, State: {state}, Action: {action}, Reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6de110-1a1d-492e-a477-ff150fa5cdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cost')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = np.arange(1,len(rewards)+1)\n",
    "plt.plot(x_axis,rewards, label='rewards')\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cea9fa-4123-43ac-81ff-e071488eac7f",
   "metadata": {},
   "source": [
    "#### TRIAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad9b295-be66-49a4-8f42-d1cc7c0f8c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [60. 62.], Action: [-0.5560657], Reward: -0.004591107368469238\n",
      "State: [66. 62.], Action: [0.01966027], Reward: -0.19463665299117588\n",
      "State: [62.7 62. ], Action: [-0.24267145], Reward: -0.15267505966127648\n",
      "State: [68.97 62.  ], Action: [0.01779892], Reward: -0.18413871890120237\n",
      "State: [75.867 62.   ], Action: [0.01473742], Reward: -0.16771253161970542\n",
      "State: [72.07365 62.     ], Action: [0.02107393], Reward: -0.22783121759436126\n",
      "State: [79.281015 62.      ], Action: [0.00963664], Reward: -0.11460044088545544\n",
      "State: [87.2091165 62.       ], Action: [-0.00178407], Reward: -0.023338037796984963\n",
      "State: [95.93002815 62.        ], Action: [-0.01624967], Reward: -0.23382466418664954\n",
      "State: [105.52303097  62.        ], Action: [-0.02216915], Reward: -0.3509034507758804\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "episode_reward = 0\n",
    "for t in range(termination_time):\n",
    "    action = ddpg_agent.get_action(state)\n",
    "    action = action\n",
    "    next_state, reward = env.step(state,action)\n",
    "    print(f'State: {state}, Action: {action}, Reward: {reward}')\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a92008-c16a-4a23-98af-63210a5a14c6",
   "metadata": {},
   "source": [
    "### American Early Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3ac2e-94e8-4f1a-932b-bac0f8893877",
   "metadata": {},
   "source": [
    "For American Put Option, your reward for excercising or not depends on \n",
    "the current payoff vs expected value of the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c9f68242-d5fb-446b-ade7-0b468745ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,1)\n",
    "        self.tanh = nn.Tanh()\n",
    "            \n",
    "    def forward(self, state):\n",
    "        action = self.tanh(self.linear1(state))\n",
    "        action = self.tanh(self.linear2(action))\n",
    "        action = self.linear3(action)\n",
    "        action = self.tanh(action)\n",
    "        return action    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim + action_dim,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,1)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, state, action):\n",
    "        input_tensor = torch.cat([state,action],1)\n",
    "        value = self.relu(self.linear1(input_tensor))\n",
    "        value = self.relu(self.linear2(value))\n",
    "        value = self.linear3(value)\n",
    "        return value\n",
    "    \n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, np.array([done]))\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "            \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "class DDPGAgent():\n",
    "    def __init__(self, state_dim, action_dim, \n",
    "                 actor_lr=1e-5, critic_lr=1e-4, \n",
    "                 gamma=0.99, tau=1e-2, max_memory=50000):\n",
    "        \n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.gamma=gamma\n",
    "        self.tau=tau\n",
    "        \n",
    "        ###Actors and Critics\n",
    "        self.actor = Actor(self.state_dim)\n",
    "        self.target_actor = Actor(self.state_dim)\n",
    "        self.critic = Critic(self.state_dim,self.action_dim)\n",
    "        self.target_critic = Critic(self.state_dim,self.action_dim)\n",
    "        \n",
    "        #Make sure the params are the same for both networks. from actor -> target_actor, critic -> target_critic\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        #For training:\n",
    "        self.replay_buffer = ReplayBuffer(max_memory)\n",
    "        self.critic_criterion = nn.MSELoss()\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        state.requires_grad=True\n",
    "        action = self.actor.forward(state)\n",
    "        action = action.detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, done = self.replay_buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        done = torch.FloatTensor(done)\n",
    "        ##Critic Update\n",
    "        # print(f'State: {states},Action: {actions}')\n",
    "        Q_values = self.critic.forward(states, actions.detach())\n",
    "        next_actions = self.target_actor.forward(next_states)\n",
    "        next_Q = self.target_critic.forward(next_states, next_actions.detach())\n",
    "        Q_targets = rewards + (1 - done) * self.gamma * next_Q\n",
    "        critic_loss = self.critic_criterion(Q_values,Q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        ####Policy Update\n",
    "        actor_loss = -self.critic.forward(states,self.actor.forward(states)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        ####Update Target Networks\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1-self.tau) * target_param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1-self.tau) * target_param.data)\n",
    "            \n",
    "class AmericanOptionExerciseEnvironment():\n",
    "    def __init__(self, strike_price,init_stock_price,up_factor,down_factor,risk_free_rate,termination_time):\n",
    "        self.strike_price=strike_price\n",
    "        self.init_stock_price=init_stock_price\n",
    "        self.up_factor=up_factor\n",
    "        self.down_factor=down_factor\n",
    "        self.risk_free_rate=risk_free_rate\n",
    "        self.hedge_portfolio_value=0\n",
    "        self.time_step = 0\n",
    "        self.termination_time=termination_time\n",
    "        self.risk_neutral_probability = (1 + risk_free_rate - down_factor) / (up_factor - down_factor)\n",
    "        \n",
    "    def reward_function(self,up_price,down_price,exercise,state):\n",
    "        up_payoff = max(strike_price - up_price,0)\n",
    "        down_payoff = max(strike_price - down_price,0)\n",
    "        current_payoff = max(strike_price - state,0)\n",
    "        value = self.risk_neutral_probability * up_payoff + (1-self.risk_neutral_probability) * down_payoff\n",
    "        expected_value = value / (1 + risk_free_rate)\n",
    "        if self.time_step == self.termination_time or bool(exercise):\n",
    "            return current_payoff\n",
    "        else:\n",
    "            return expected_value - current_payoff\n",
    "    \n",
    "    def reset(self):\n",
    "        self.time_step = 0\n",
    "        init_state = np.array([self.init_stock_price,self.strike_price])\n",
    "        return init_state\n",
    "    \n",
    "    def step(self,state,action):\n",
    "        self.time_step += 1\n",
    "        exercise = action.item() >= 0.0\n",
    "        up_price = self.up_factor * state[0].item()\n",
    "        down_price = self.down_factor * state[0].item()\n",
    "        next_price = np.random.choice([up_price, down_price],\n",
    "                                     p=[self.risk_neutral_probability,1-self.risk_neutral_probability])\n",
    "        next_state=np.array([next_price,self.strike_price])\n",
    "        reward = self.reward_function(up_price,down_price,exercise,state[0].item())\n",
    "        # print(f'State: {state}, Reward: {reward}, Action: {action}')\n",
    "        return next_state, reward, float(exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "fa2bef9c-fde8-4507-9b63-795b5b9820b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0, State: [54. 62.], Action: [0.87844515], Reward: 2.0\n",
      "Episode:500, State: [54. 62.], Action: [1.98767586], Reward: 2.0\n",
      "Episode:1000, State: [71.874 62.   ], Action: [0.22834033], Reward: 0\n",
      "Episode:1500, State: [79.86 62.  ], Action: [0.01185499], Reward: 0\n",
      "Episode:2000, State: [66. 62.], Action: [0.02026904], Reward: 2.0\n",
      "Episode:2500, State: [54. 62.], Action: [0.00739247], Reward: 2.0\n",
      "Episode:3000, State: [59.4 62. ], Action: [0.00072145], Reward: 0\n",
      "Episode:3500, State: [66. 62.], Action: [0.00083412], Reward: 2.0\n",
      "Episode:4000, State: [72.6 62. ], Action: [0.00016049], Reward: 0\n",
      "Episode:4500, State: [48.6 62. ], Action: [1.99875206], Reward: 8.0\n",
      "Episode:5000, State: [65.34 62.  ], Action: [0.00027382], Reward: 2.5999999999999943\n",
      "Episode:5500, State: [79.86 62.  ], Action: [1.00957217e-05], Reward: 0\n",
      "Episode:6000, State: [65.34 62.  ], Action: [4.58888847e-06], Reward: 0\n",
      "Episode:6500, State: [48.6 62. ], Action: [0.18373797], Reward: 8.0\n",
      "Episode:7000, State: [59.4 62. ], Action: [1.9999699], Reward: 8.0\n",
      "Episode:7500, State: [58.806 62.   ], Action: [1.99999344], Reward: 8.54\n",
      "Episode:8000, State: [59.4 62. ], Action: [1.78811256e-07], Reward: 0\n",
      "Episode:8500, State: [58.21794 62.     ], Action: [5.85944376e-08], Reward: 0\n",
      "Episode:9000, State: [54. 62.], Action: [1.19208648e-07], Reward: 2.0\n",
      "Episode:9500, State: [59.4 62. ], Action: [1.99999819], Reward: 8.0\n",
      "Episode:10000, State: [69.73927033 62.        ], Action: [-1.97585988], Reward: 0\n"
     ]
    }
   ],
   "source": [
    "state_dim=2\n",
    "action_dim=1\n",
    "strike_price=62.0\n",
    "init_stock_price=60.0\n",
    "up_factor=1.1\n",
    "down_factor=0.90\n",
    "risk_free_rate=0.03\n",
    "termination_time=10\n",
    "episodes=10000\n",
    "batch_size=64\n",
    "\n",
    "exercise_env = AmericanOptionExerciseEnvironment(strike_price,init_stock_price,up_factor,down_factor,\n",
    "                                         risk_free_rate,termination_time)\n",
    "ddpg_agent = DDPGAgent(state_dim,action_dim,\n",
    "                       actor_lr=5e-6,critic_lr=2e-4)\n",
    "\n",
    "rewards = []\n",
    "for e in range(episodes+1):\n",
    "    state = exercise_env.reset()\n",
    "    episode_reward = 0\n",
    "    for t in range(termination_time):\n",
    "        action = ddpg_agent.get_action(state)\n",
    "        noise = np.random.normal(0,1, size=action_dim)\n",
    "        action = action + (np.tanh(noise * 10)) \n",
    "        next_state, reward, done = exercise_env.step(state,action)\n",
    "        ddpg_agent.replay_buffer.push(state,action,reward,next_state,done)\n",
    "        if len(ddpg_agent.replay_buffer) > batch_size:\n",
    "            ddpg_agent.update(batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    rewards.append(episode_reward)\n",
    "    if e % 500 == 0:\n",
    "        print(f'Episode:{e}, State: {state}, Action: {action}, Reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "755655f5-2cf5-4218-b14e-0f6f26d2638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [60. 62.], Action: [-1.], Reward: 0.718446601941749\n",
      "State: [54. 62.], Action: [0.9999999], Reward: 8.0\n"
     ]
    }
   ],
   "source": [
    "state = exercise_env.reset()\n",
    "episode_reward = 0\n",
    "for t in range(10):\n",
    "    action = ddpg_agent.get_action(state)\n",
    "    action = action\n",
    "    next_state, reward, done = exercise_env.step(state,action)\n",
    "    print(f'State: {state}, Action: {action}, Reward: {reward}')\n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5b105-260c-4c2b-9496-032266eaca05",
   "metadata": {},
   "source": [
    "### Analytical Answer for Optimal Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "53f94bd7-f48d-4c3e-96ba-8f2a8c45bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price: 51.87218454000002, Payoff: 10.12781545999998, Value: 8.321990217281535\n",
      "Price: 42.44087826000001, Payoff: 19.559121739999988, Value: 17.753296497281536\n",
      "Price: 34.72435494000002, Payoff: 27.27564505999998, Value: 25.46981981728154\n",
      "Price: 28.410835860000006, Payoff: 33.589164139999994, Value: 31.783338897281542\n",
      "Price: 23.245229340000005, Payoff: 38.75477065999999, Value: 36.94894541728154\n",
      "Price: 57.63576060000001, Payoff: 4.364239399999988, Value: 3.8873251882316806\n",
      "Price: 47.15653140000001, Payoff: 14.843468599999987, Value: 11.284414966292754\n",
      "Price: 38.58261660000001, Payoff: 23.41738339999999, Value: 19.858329766292755\n",
      "Price: 31.567595400000016, Payoff: 30.432404599999984, Value: 26.873350966292758\n",
      "Price: 25.828032600000004, Payoff: 36.1719674, Value: 32.61291376629276\n",
      "Price: 52.39614600000001, Payoff: 9.603853999999991, Value: 6.287676320925299\n",
      "Price: 42.86957400000001, Payoff: 19.130425999999993, Value: 13.86920887989588\n",
      "Price: 35.075106000000005, Payoff: 26.924893999999995, Value: 21.66367687989588\n",
      "Price: 28.69781400000001, Payoff: 33.30218599999999, Value: 28.040968879895885\n",
      "Price: 58.21794000000003, Payoff: 3.782059999999973, Value: 3.1973814241966325\n",
      "Price: 47.63286, Payoff: 14.36714, Value: 8.680789045208742\n",
      "Price: 38.97234, Payoff: 23.027659999999997, Value: 16.113856970772698\n",
      "Price: 31.886460000000003, Payoff: 30.113539999999997, Value: 23.199736970772705\n",
      "Price: 52.92540000000002, Payoff: 9.074599999999982, Value: 4.967547661699875\n",
      "Price: 43.3026, Payoff: 18.697400000000002, Value: 10.953750309860316\n",
      "Price: 35.4294, Payoff: 26.5706, Value: 18.052344631818155\n",
      "Price: 58.80600000000002, Payoff: 3.1939999999999813, Value: 2.649846063555103\n",
      "Price: 48.11400000000001, Payoff: 13.885999999999989, Value: 6.857008338403913\n",
      "Price: 39.36599999999999, Payoff: 22.634000000000007, Value: 13.046852740335495\n",
      "Price: 53.460000000000015, Payoff: 8.539999999999985, Value: 4.002284329856492\n",
      "Price: 43.74000000000001, Payoff: 18.25999999999999, Value: 8.760634834058223\n",
      "Price: 59.40000000000001, Payoff: 2.599999999999987, Value: 2.204167528800343\n",
      "Price: 48.60000000000001, Payoff: 13.399999999999991, Value: 5.50262816148262\n",
      "Price: 54.00000000000001, Payoff: 7.999999999999993, Value: 3.260804611882661\n"
     ]
    }
   ],
   "source": [
    "'''Use backward induction to calculate for a given set of params, which prices should you exercise at'''\n",
    "def find_optimal_exercise(price_pairs, p, strike_price):\n",
    "    exercise_prices = []\n",
    "    prev_price_pairs = []\n",
    "    for pair in price_pairs:\n",
    "        up_payoff = pair[0][0][1]\n",
    "        down_payoff = pair[0][1][1]\n",
    "        value = p * up_payoff + (1-p) * down_payoff\n",
    "        value = value / (1+risk_free_rate)\n",
    "        prev_price = pair[0][0][0]/up_factor\n",
    "        prev_price_pairs.append((prev_price,value))\n",
    "        if max(strike_price - prev_price, 0) > value:\n",
    "            print(f'Price: {prev_price}, Payoff: {max(strike_price - prev_price, 0)}, Value: {value}')\n",
    "            exercise_prices.append(prev_price)\n",
    "\n",
    "    return np.lib.stride_tricks.sliding_window_view(np.array(prev_price_pairs), window_shape = (2,2), axis=(0,1)), \\\n",
    "           exercise_prices\n",
    "\n",
    "def binomial_american_optimal_exercise(current_price, strike_price, risk_free_rate, termination_time, up_factor, down_factor):\n",
    "    #Risk-neutral probability - p\n",
    "    exercise_prices = []\n",
    "    p = ((1+risk_free_rate) - down_factor)/(up_factor - down_factor)\n",
    "    final_prices = []\n",
    "    for t in range(termination_time+1):\n",
    "        price = current_price * (up_factor ** (termination_time-t)) * (down_factor ** (t))\n",
    "        payoff = max(strike_price - price, 0)\n",
    "        final_prices.append((price,payoff))\n",
    "    prices = np.lib.stride_tricks.sliding_window_view(np.array(final_prices), window_shape = (2,2), axis=(0,1))\n",
    "    \n",
    "    while len(prices) != 1:\n",
    "        prices, exe_prices = find_optimal_exercise(prices,p,strike_price)\n",
    "        exercise_prices.extend(exe_prices)\n",
    "    \n",
    "    return exercise_prices\n",
    "\n",
    "optimal_exercise_prices = np.array(binomial_american_optimal_exercise(init_stock_price,strike_price,risk_free_rate,\n",
    "                                                                      termination_time,up_factor,down_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "71c968b6-d208-426d-8ae5-c928578dc545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [51.87218454 62.        ], Action: [1.]\n",
      "State: [42.44087826 62.        ], Action: [1.]\n",
      "State: [34.72435494 62.        ], Action: [1.]\n",
      "State: [28.41083586 62.        ], Action: [1.]\n",
      "State: [23.24522934 62.        ], Action: [1.]\n",
      "State: [57.6357606 62.       ], Action: [-0.9999906]\n",
      "State: [47.1565314 62.       ], Action: [1.]\n",
      "State: [38.5826166 62.       ], Action: [1.]\n",
      "State: [31.5675954 62.       ], Action: [1.]\n",
      "State: [25.8280326 62.       ], Action: [1.]\n",
      "State: [52.396146 62.      ], Action: [1.]\n",
      "State: [42.869574 62.      ], Action: [1.]\n",
      "State: [35.075106 62.      ], Action: [1.]\n",
      "State: [28.697814 62.      ], Action: [1.]\n",
      "State: [58.21794 62.     ], Action: [-0.99999934]\n",
      "State: [47.63286 62.     ], Action: [1.]\n",
      "State: [38.97234 62.     ], Action: [1.]\n",
      "State: [31.88646 62.     ], Action: [1.]\n",
      "State: [52.9254 62.    ], Action: [1.]\n",
      "State: [43.3026 62.    ], Action: [1.]\n",
      "State: [35.4294 62.    ], Action: [1.]\n",
      "State: [58.806 62.   ], Action: [-0.9999999]\n",
      "State: [48.114 62.   ], Action: [1.]\n",
      "State: [39.366 62.   ], Action: [1.]\n",
      "State: [53.46 62.  ], Action: [1.]\n",
      "State: [43.74 62.  ], Action: [1.]\n",
      "State: [59.4 62. ], Action: [-1.]\n",
      "State: [48.6 62. ], Action: [1.]\n",
      "State: [54. 62.], Action: [0.9999999]\n"
     ]
    }
   ],
   "source": [
    "optimal_states = [ np.array([x , strike_price]) for x in optimal_exercise_prices]\n",
    "actions = [ ddpg_agent.get_action(s) for s in optimal_states ]\n",
    "for s, a in zip(optimal_states, actions):\n",
    "    print(f'State: {s}, Action: {a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc56e3e-daea-448a-9727-d121c69b2e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
