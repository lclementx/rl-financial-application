{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203379bc-c0e9-4c32-b6de-534efb14ef07",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### MAIN: In a Binomial model of a single stock with non-zero interest rate, assume that we can hedge any fraction of a stock, use policy gradient to train the optimal policy of hedging an ATM American put option with maturity T = 10. WHEN do you early exercise the option? Is your solution same as what you obtain from delta hedging?\n",
    "\n",
    "#### OPTIONAL: For really advanced students: supppose the stock follows a GBM (Geometric Brownian Motion), construct an algorithm to train a Neural Network that hedges an ATM American Put Option\n",
    "\n",
    "#### OPTIONAL BONUS: After solving the optional question, sue the Soft Actor Critic algorithm in TF agent to solve the problem again in the colab.resarch.google.com environment. Compare your results\n",
    "\n",
    "#### ADVANCED EXTRA BONUS: Implement the GAC algorithm using the TF agent library and solve the optinal problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269007c7-475a-49e9-821b-ccf70276c46b",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b59a1a2-3c83-4409-96d1-fc6947dd9edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clemmie/opt/anaconda3/envs/Python3_7_Plus_R/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import MultivariateNormal, Normal\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.special import comb\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c11c39-72c8-4d97-a207-3ff119007479",
   "metadata": {},
   "source": [
    "### 10 Step American Put Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73297ed2-e76a-4a59-b005-c4946670fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,1)\n",
    "        self.tanh = nn.Tanh()\n",
    "            \n",
    "    def forward(self, state):\n",
    "        action = self.tanh(self.linear1(state))\n",
    "        action = self.tanh(self.linear2(action))\n",
    "        action = self.linear3(action)\n",
    "        return action    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim + action_dim,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,1)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, state, action):\n",
    "        input_tensor = torch.cat([state,action],1)\n",
    "        value = self.relu(self.linear1(input_tensor))\n",
    "        value = self.relu(self.linear2(value))\n",
    "        value = self.linear3(value)\n",
    "        return value\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, np.array([done]))\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "            \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DDPGAgent():\n",
    "    def __init__(self, state_dim, action_dim, \n",
    "                 actor_lr=1e-4, critic_lr=1e-3, \n",
    "                 gamma=0.99, tau=1e-2, max_memory=50000):\n",
    "        \n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.gamma=gamma\n",
    "        self.tau=tau\n",
    "        \n",
    "        ###Actors and Critics\n",
    "        self.actor = Actor(self.state_dim)\n",
    "        self.target_actor = Actor(self.state_dim)\n",
    "        self.critic = Critic(self.state_dim,self.action_dim)\n",
    "        self.target_critic = Critic(self.state_dim,self.action_dim)\n",
    "        \n",
    "        #Make sure the params are the same for both networks. from actor -> target_actor, critic -> target_critic\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        #For training:\n",
    "        self.replay_buffer = ReplayBuffer(max_memory)\n",
    "        self.critic_criterion = nn.MSELoss()\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        state.requires_grad=True\n",
    "        action = self.actor.forward(state)\n",
    "        action = action.detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, done = self.replay_buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        done = torch.FloatTensor(done)\n",
    "        ##Critic Update\n",
    "        # print(f'State: {states},Action: {actions}')\n",
    "        Q_values = self.critic.forward(states, actions.detach())\n",
    "        next_actions = self.target_actor.forward(next_states)\n",
    "        next_Q = self.target_critic.forward(next_states, next_actions.detach())\n",
    "        Q_targets = rewards + (1 - done) * self.gamma * next_Q\n",
    "        critic_loss = self.critic_criterion(Q_values,Q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        ####Policy Update\n",
    "        actor_loss = -self.critic.forward(states,self.actor.forward(states)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        ####Update Target Networks\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1-self.tau) * target_param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1-self.tau) * target_param.data)\n",
    "    \n",
    "class AmericanOptionHedgingEnvironment():\n",
    "    def __init__(self, strike_price,init_stock_price,up_factor,down_factor,risk_free_rate,termination_time):\n",
    "        self.strike_price=strike_price\n",
    "        self.init_stock_price=init_stock_price\n",
    "        self.up_factor=up_factor\n",
    "        self.down_factor=down_factor\n",
    "        self.risk_free_rate=risk_free_rate\n",
    "        self.hedge_portfolio_value=0\n",
    "        self.time_step = 0\n",
    "        self.termination_time=termination_time\n",
    "        self.risk_neutral_probability = (1 + risk_free_rate - down_factor) / (up_factor - down_factor)\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        if self.time_step == self.termination_time:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "    def reward_function(self,up_price,down_price,action,state):\n",
    "        up_payoff = max(self.strike_price - up_price,0)\n",
    "        down_payoff = max(self.strike_price - down_price,0)\n",
    "        up_hedge = action * (up_price - state) - up_payoff\n",
    "        down_hedge = action * (down_price - state) - down_payoff\n",
    "        return -abs(up_hedge - down_hedge)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.time_step = 0\n",
    "        init_state = np.array([self.init_stock_price,self.strike_price])\n",
    "        return init_state\n",
    "    \n",
    "    def step(self,state,action):\n",
    "        self.time_step += 1\n",
    "        up_price = self.up_factor * state[0].item()\n",
    "        down_price = self.down_factor * state[0].item()\n",
    "        next_price = np.random.choice([up_price, down_price],\n",
    "                                     p=[self.risk_neutral_probability,1-self.risk_neutral_probability])\n",
    "        next_state=np.array([next_price,self.strike_price])\n",
    "        reward = self.reward_function(up_price,down_price,action.item(),state[0].item())\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349da8e1-0faf-4373-846f-d0472a5ae9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0, State: [74.77091626 62.        ], Action: [-0.47544874], Reward: -5.613116468966651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clemmie/opt/anaconda3/envs/Python3_7_Plus_R/lib/python3.7/site-packages/ipykernel_launcher.py:98: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/miniforge3/conda-bld/pytorch-recipe_1660136152924/work/torch/csrc/utils/tensor_new.cpp:204.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:500, State: [116.07533406  62.        ], Action: [0.07517658], Reward: -1.1899290637598416\n",
      "Episode:1000, State: [48.16432331 62.        ], Action: [-0.96587489], Reward: -0.2595178153026332\n",
      "Episode:1500, State: [116.07533406  62.        ], Action: [-0.24437949], Reward: -4.478910164262164\n",
      "Episode:2000, State: [64.57488222 62.        ], Action: [-0.62501023], Reward: -0.7271523099800774\n",
      "Episode:2500, State: [55.76921647 62.        ], Action: [-0.65741473], Reward: -0.44180915120404407\n",
      "Episode:3000, State: [134.40301839  62.        ], Action: [-0.05131891], Reward: -0.9405567449704332\n",
      "Episode:3500, State: [86.57685041 62.        ], Action: [-0.05502739], Reward: -0.6496497543348329\n",
      "Episode:4000, State: [100.24687942  62.        ], Action: [-0.02479312], Reward: -0.33892265673597544\n",
      "Episode:4500, State: [74.77091626 62.        ], Action: [-0.03590803], Reward: -0.3661195076146288\n",
      "Episode:5000, State: [74.77091626 62.        ], Action: [0.02849622], Reward: -0.336424445191722\n"
     ]
    }
   ],
   "source": [
    "state_dim=2\n",
    "action_dim=1\n",
    "strike_price=62.0\n",
    "init_stock_price=60.0\n",
    "up_factor=1.1\n",
    "down_factor=0.95\n",
    "risk_free_rate=0.03\n",
    "termination_time=10\n",
    "episodes=5000\n",
    "batch_size=64\n",
    "#https://arxiv.org/pdf/1911.11679.pdf - becareful of actor_learning_rate vs critic_learning_rate\n",
    "env = AmericanOptionHedgingEnvironment(strike_price,init_stock_price,up_factor,down_factor,risk_free_rate,termination_time)\n",
    "ddpg_agent = DDPGAgent(state_dim,action_dim,\n",
    "                       actor_lr=4e-5,critic_lr=2e-4)\n",
    "\n",
    "rewards = []\n",
    "for e in range(episodes+1):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for t in range(termination_time):\n",
    "        action = ddpg_agent.get_action(state)\n",
    "        noise = np.random.normal(0,1, size=action_dim) / (1 + e/100) #Progressively reduce noise as we learn\n",
    "        action = action + noise \n",
    "        next_state, reward = env.step(state,action)\n",
    "        ddpg_agent.replay_buffer.push(state,action,reward,next_state,env.is_terminal())\n",
    "        if len(ddpg_agent.replay_buffer) > batch_size:\n",
    "            ddpg_agent.update(batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "    if e % 500 == 0:\n",
    "        print(f'Episode:{e}, State: {state}, Action: {action}, Reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6de110-1a1d-492e-a477-ff150fa5cdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Negative Total Reward vs Episodes')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = np.arange(1,len(rewards)+1)\n",
    "plt.plot(x_axis,rewards, label='Rewards')\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Negative Total Reward')\n",
    "plt.title('Negative Total Reward vs Episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cea9fa-4123-43ac-81ff-e071488eac7f",
   "metadata": {},
   "source": [
    "#### TRIAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ad9b295-be66-49a4-8f42-d1cc7c0f8c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [60. 62.], Action: [-0.5611225], Reward: -0.050102293491363525\n",
      "State: [57. 62.], Action: [-0.9162303], Reward: -0.016230756044385508\n",
      "State: [62.7 62. ], Action: [-0.25509408], Reward: -0.03584016665816159\n",
      "State: [68.97 62.  ], Action: [0.00356483], Reward: -0.03687991921976214\n",
      "State: [65.5215 62.    ], Action: [-0.0083449], Reward: -0.08201551684308811\n",
      "State: [72.07365 62.     ], Action: [0.00414082], Reward: -0.044766565264184086\n",
      "State: [79.281015 62.      ], Action: [0.00068752], Reward: -0.008176059523025534\n",
      "State: [87.2091165 62.       ], Action: [0.01162189], Reward: -0.1520301484588976\n",
      "State: [82.84866068 62.        ], Action: [0.00476126], Reward: -0.05916955585895714\n",
      "State: [91.13352674 62.        ], Action: [0.00393181], Reward: -0.05374799698482316\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "episode_reward = 0\n",
    "for t in range(termination_time):\n",
    "    action = ddpg_agent.get_action(state)\n",
    "    action = action\n",
    "    next_state, reward = env.step(state,action)\n",
    "    print(f'State: {state}, Action: {action}, Reward: {reward}')\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a92008-c16a-4a23-98af-63210a5a14c6",
   "metadata": {},
   "source": [
    "### American Early Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3ac2e-94e8-4f1a-932b-bac0f8893877",
   "metadata": {},
   "source": [
    "For American Put Option, your reward for excercising or not depends on \n",
    "the current payoff vs expected value of the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f68242-d5fb-446b-ade7-0b468745ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,1)\n",
    "        self.tanh = nn.Tanh()\n",
    "            \n",
    "    def forward(self, state):\n",
    "        action = self.tanh(self.linear1(state))\n",
    "        action = self.tanh(self.linear2(action))\n",
    "        action = self.linear3(action)\n",
    "        action = self.tanh(action)\n",
    "        return action    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim + action_dim,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,1)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, state, action):\n",
    "        input_tensor = torch.cat([state,action],1)\n",
    "        value = self.relu(self.linear1(input_tensor))\n",
    "        value = self.relu(self.linear2(value))\n",
    "        value = self.linear3(value)\n",
    "        return value\n",
    "    \n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, np.array([done]))\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "            \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "class DDPGAgent():\n",
    "    def __init__(self, state_dim, action_dim, \n",
    "                 actor_lr=1e-5, critic_lr=1e-4, \n",
    "                 gamma=0.99, tau=1e-2, max_memory=50000):\n",
    "        \n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.gamma=gamma\n",
    "        self.tau=tau\n",
    "        \n",
    "        ###Actors and Critics\n",
    "        self.actor = Actor(self.state_dim)\n",
    "        self.target_actor = Actor(self.state_dim)\n",
    "        self.critic = Critic(self.state_dim,self.action_dim)\n",
    "        self.target_critic = Critic(self.state_dim,self.action_dim)\n",
    "        \n",
    "        #Make sure the params are the same for both networks. from actor -> target_actor, critic -> target_critic\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        #For training:\n",
    "        self.replay_buffer = ReplayBuffer(max_memory)\n",
    "        self.critic_criterion = nn.MSELoss()\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        state.requires_grad=True\n",
    "        action = self.actor.forward(state)\n",
    "        action = action.detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, done = self.replay_buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        done = torch.FloatTensor(done)\n",
    "        ##Critic Update\n",
    "        # print(f'State: {states},Action: {actions}')\n",
    "        Q_values = self.critic.forward(states, actions.detach())\n",
    "        next_actions = self.target_actor.forward(next_states)\n",
    "        next_Q = self.target_critic.forward(next_states, next_actions.detach())\n",
    "        Q_targets = rewards + (1 - done) * self.gamma * next_Q\n",
    "        critic_loss = self.critic_criterion(Q_values,Q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        ####Policy Update\n",
    "        actor_loss = -self.critic.forward(states,self.actor.forward(states)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        ####Update Target Networks\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1-self.tau) * target_param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1-self.tau) * target_param.data)\n",
    "            \n",
    "class AmericanOptionExerciseEnvironment():\n",
    "    def __init__(self, strike_price,init_stock_price,up_factor,down_factor,risk_free_rate,termination_time):\n",
    "        self.strike_price=strike_price\n",
    "        self.init_stock_price=init_stock_price\n",
    "        self.up_factor=up_factor\n",
    "        self.down_factor=down_factor\n",
    "        self.risk_free_rate=risk_free_rate\n",
    "        self.hedge_portfolio_value=0\n",
    "        self.time_step = 0\n",
    "        self.termination_time=termination_time\n",
    "        self.risk_neutral_probability = (1 + risk_free_rate - down_factor) / (up_factor - down_factor)\n",
    "        \n",
    "    def reward_function(self,up_price,down_price,exercise,state):\n",
    "        up_payoff = max(strike_price - up_price,0)\n",
    "        down_payoff = max(strike_price - down_price,0)\n",
    "        current_payoff = max(strike_price - state,0)\n",
    "        value = self.risk_neutral_probability * up_payoff + (1-self.risk_neutral_probability) * down_payoff\n",
    "        expected_value = value / (1 + risk_free_rate)\n",
    "        if self.time_step == self.termination_time or bool(exercise):\n",
    "            return current_payoff\n",
    "        else:\n",
    "            return expected_value - current_payoff\n",
    "    \n",
    "    def reset(self):\n",
    "        self.time_step = 0\n",
    "        init_state = np.array([self.init_stock_price,self.strike_price])\n",
    "        return init_state\n",
    "    \n",
    "    def step(self,state,action):\n",
    "        self.time_step += 1\n",
    "        exercise = action.item() >= 0.0\n",
    "        up_price = self.up_factor * state[0].item()\n",
    "        down_price = self.down_factor * state[0].item()\n",
    "        next_price = np.random.choice([up_price, down_price],\n",
    "                                     p=[self.risk_neutral_probability,1-self.risk_neutral_probability])\n",
    "        next_state=np.array([next_price,self.strike_price])\n",
    "        reward = self.reward_function(up_price,down_price,exercise,state[0].item())\n",
    "        # print(f'State: {state}, Reward: {reward}, Action: {action}')\n",
    "        return next_state, reward, float(exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2bef9c-fde8-4507-9b63-795b5b9820b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0, State: [65.34 62.  ], Action: [0.54052216], Reward: 2.5999999999999943\n",
      "Episode:500, State: [48.6 62. ], Action: [1.9706921], Reward: 8.0\n",
      "Episode:1000, State: [64.039734 62.      ], Action: [0.05499083], Reward: 0\n",
      "Episode:1500, State: [59.4 62. ], Action: [1.89132534], Reward: 8.0\n",
      "Episode:2000, State: [66. 62.], Action: [0.00898558], Reward: 2.0\n",
      "Episode:2500, State: [72.6 62. ], Action: [0.00079272], Reward: 0\n",
      "Episode:3000, State: [64.6866 62.    ], Action: [0.00026655], Reward: 0\n",
      "Episode:3500, State: [79.0614 62.    ], Action: [0.00014362], Reward: 0\n",
      "Episode:4000, State: [59.4 62. ], Action: [1.99844399], Reward: 8.0\n",
      "Episode:4500, State: [54. 62.], Action: [0.00016817], Reward: 2.0\n",
      "Episode:5000, State: [54. 62.], Action: [7.82012939e-05], Reward: 2.0\n",
      "Episode:5500, State: [52.9254 62.    ], Action: [0.00019413], Reward: 3.1939999999999955\n",
      "Episode:6000, State: [59.4 62. ], Action: [1.99992704], Reward: 8.0\n",
      "Episode:6500, State: [87.846 62.   ], Action: [5.36441517e-07], Reward: 0\n",
      "Episode:7000, State: [71.15526 62.     ], Action: [2.05552913e-07], Reward: 0\n",
      "Episode:7500, State: [79.86 62.  ], Action: [5.96046446e-08], Reward: 0\n",
      "Episode:8000, State: [48.6 62. ], Action: [1.99999765], Reward: 8.0\n",
      "Episode:8500, State: [59.4 62. ], Action: [6.64707738e-07], Reward: 8.0\n",
      "Episode:9000, State: [85.23688595 62.        ], Action: [-1.91167464], Reward: 0\n",
      "Episode:9500, State: [71.874 62.   ], Action: [0.], Reward: 0\n",
      "Episode:10000, State: [104.17841617  62.        ], Action: [-2.], Reward: 0\n"
     ]
    }
   ],
   "source": [
    "state_dim=2\n",
    "action_dim=1\n",
    "strike_price=62.0\n",
    "init_stock_price=60.0\n",
    "up_factor=1.1\n",
    "down_factor=0.90\n",
    "risk_free_rate=0.03\n",
    "termination_time=10\n",
    "episodes=10000\n",
    "batch_size=64\n",
    "\n",
    "exercise_env = AmericanOptionExerciseEnvironment(strike_price,init_stock_price,up_factor,down_factor,\n",
    "                                         risk_free_rate,termination_time)\n",
    "ddpg_exercise_agent = DDPGAgent(state_dim,action_dim,\n",
    "                       actor_lr=5e-6,critic_lr=2e-4)\n",
    "\n",
    "rewards = []\n",
    "for e in range(episodes+1):\n",
    "    state = exercise_env.reset()\n",
    "    episode_reward = 0\n",
    "    for t in range(termination_time):\n",
    "        action = ddpg_exercise_agent.get_action(state)\n",
    "        noise = np.random.normal(0,1, size=action_dim)\n",
    "        action = action + (np.tanh(noise * 10)) \n",
    "        next_state, reward, done = exercise_env.step(state,action)\n",
    "        ddpg_exercise_agent.replay_buffer.push(state,action,reward,next_state,done)\n",
    "        if len(ddpg_exercise_agent.replay_buffer) > batch_size:\n",
    "            ddpg_exercise_agent.update(batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    rewards.append(episode_reward)\n",
    "    if e % 500 == 0:\n",
    "        print(f'Episode:{e}, State: {state}, Action: {action}, Reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "755655f5-2cf5-4218-b14e-0f6f26d2638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [60. 62.], Action: [-1.], Reward: 0.718446601941749\n",
      "State: [54. 62.], Action: [0.99999994], Reward: 8.0\n"
     ]
    }
   ],
   "source": [
    "state = exercise_env.reset()\n",
    "episode_reward = 0\n",
    "for t in range(10):\n",
    "    action = ddpg_exercise_agent.get_action(state)\n",
    "    action = action\n",
    "    next_state, reward, done = exercise_env.step(state,action)\n",
    "    print(f'State: {state}, Action: {action}, Reward: {reward}')\n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5b105-260c-4c2b-9496-032266eaca05",
   "metadata": {},
   "source": [
    "### Analytical Answer for Optimal Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53f94bd7-f48d-4c3e-96ba-8f2a8c45bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculate for a given set of params, which prices should you exercise at'''\n",
    "def find_optimal_exercise(price_pairs, p, strike_price):\n",
    "    exercise_prices = []\n",
    "    prev_price_pairs = []\n",
    "    state_price_value = {}\n",
    "    for pair in price_pairs:\n",
    "        up_payoff = pair[0][0][1]\n",
    "        down_payoff = pair[0][1][1]\n",
    "        value = p * up_payoff + (1-p) * down_payoff\n",
    "        value = value / (1+risk_free_rate)\n",
    "        prev_price = pair[0][0][0]/up_factor\n",
    "        prev_price_pairs.append((prev_price,value))\n",
    "        state_price_value[prev_price] = value\n",
    "        if max(strike_price - prev_price, 0) > value:\n",
    "            # print(f'Price: {prev_price}, Payoff: {max(strike_price - prev_price, 0)}, Value: {value}')\n",
    "            exercise_prices.append(prev_price)\n",
    "\n",
    "    return np.lib.stride_tricks.sliding_window_view(np.array(prev_price_pairs), window_shape = (2,2), axis=(0,1)), \\\n",
    "           exercise_prices, state_price_value\n",
    "\n",
    "def binomial_american_optimal_exercise(current_price, strike_price, risk_free_rate, termination_time, up_factor, down_factor):\n",
    "    #Risk-neutral probability - p\n",
    "    exercise_prices = []\n",
    "    all_prices = []\n",
    "    all_price_values = {}\n",
    "    p = ((1+risk_free_rate) - down_factor)/(up_factor - down_factor)\n",
    "    final_prices = []\n",
    "    for t in range(termination_time+1):\n",
    "        price = current_price * (up_factor ** (termination_time-t)) * (down_factor ** (t))\n",
    "        payoff = max(strike_price - price, 0)\n",
    "        final_prices.append((price,payoff))\n",
    "    prices = np.lib.stride_tricks.sliding_window_view(np.array(final_prices), window_shape = (2,2), axis=(0,1))\n",
    "    \n",
    "    while len(prices) != 1:\n",
    "        prices, exe_prices, price_vals = find_optimal_exercise(prices,p,strike_price)\n",
    "        exercise_prices.extend(exe_prices)\n",
    "        all_price_values = {**all_price_values,**price_vals}\n",
    "        all_prices.extend(prices)\n",
    "    \n",
    "    return all_prices, exercise_prices, all_price_values\n",
    "\n",
    "all_prices, optimal_exercise_prices, all_price_values = \\\n",
    "    binomial_american_optimal_exercise(init_stock_price,strike_price,\n",
    "                                       risk_free_rate,termination_time,up_factor,down_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71c968b6-d208-426d-8ae5-c928578dc545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed Optimal Exercise Prices\n",
      "Price: 57.63576060000001, Action: -0.9999985098838806, Payoff: 4.364239399999988, Expected Value: 3.8873251882316806\n",
      "Price: 58.21794000000003, Action: -0.9999998807907104, Payoff: 3.782059999999973, Expected Value: 3.1973814241966325\n",
      "Price: 58.80600000000002, Action: -1.0, Payoff: 3.1939999999999813, Expected Value: 2.649846063555103\n",
      "Price: 59.40000000000001, Action: -1.0, Payoff: 2.599999999999987, Expected Value: 2.204167528800343\n"
     ]
    }
   ],
   "source": [
    "### FOR ALL STATES - find out which states my policy exercises and compare it with the analytical answer\n",
    "all_prices_flatten = np.unique(np.array([x[0,:,0] for x in all_prices]).flatten())\n",
    "all_states = [ np.array([x , strike_price]) for x in all_prices_flatten]\n",
    "actions = [ ddpg_exercise_agent.get_action(s) for s in all_states ]\n",
    "optimal_exercise_prices = np.array(optimal_exercise_prices)\n",
    "\n",
    "#Find the common ones with the optimal list\n",
    "correnct_optimal_exercise_prices = []\n",
    "missed_optimal_exercise_prices = []\n",
    "for s, a in zip(all_states, actions):\n",
    "    if a[0] > 0:\n",
    "        if np.isin(s[0],optimal_exercise_prices):\n",
    "            correnct_optimal_exercise_prices.append(s[0])\n",
    "        else:\n",
    "            print(f'Wrongly Exercising: {s[0]}')\n",
    "    else:\n",
    "        if np.isin(s[0],optimal_exercise_prices):\n",
    "            payoff = max(strike_price - s[0], 0)\n",
    "            val = all_price_values[s[0]]\n",
    "            missed_optimal_exercise_prices.append((s[0],a[0],payoff,val))\n",
    "\n",
    "\n",
    "print(f'Missed Optimal Exercise Prices')\n",
    "for price, action, payoff, val in missed_optimal_exercise_prices:\n",
    "    print(f'Price: {price}, Action: {action}, Payoff: {payoff}, Expected Value: {val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ef0fd-78b5-4d97-8655-ca4a4a412e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
